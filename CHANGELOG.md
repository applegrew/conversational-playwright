# Changelog

## Version 1.1.0 - Stability and UX Improvements (2025-11-06)

### Major Improvements

#### MCP Server Connection
- âœ… **Fixed MCP server connection** - Now uses SSE (Server-Sent Events) transport over HTTP
- âœ… **HTTP endpoint detection** - Actively polls server endpoint to detect readiness
- âœ… **Port conflict resolution** - Automatically kills existing processes on port 3000
- âœ… **Proper cleanup** - Server process cleanup on app exit and force-quit
- âœ… **Configurable port** - Set `MCP_SERVER_PORT` in `.env`

#### Logging System
- âœ… **Configurable log levels** - 5 levels: ERROR, WARN, INFO, DEBUG, VERBOSE
- âœ… **Smart log filtering** - Screenshot streaming (15 FPS) only logs at VERBOSE level
- âœ… **Reduced noise** - LLM tool calls log at INFO, streaming at VERBOSE
- âœ… **Environment control** - Set `LOG_LEVEL` in `.env`
- âœ… **Logger utility** - Centralized logging via `src/utils/logger.js`

#### Error Handling
- âœ… **User-friendly error messages** - No more raw JSON dumps in chat
- âœ… **Retry button** - Click â†» Retry to resend failed messages
- âœ… **Error parsing** - Extracts status codes (429, 401, etc.) from errors
- âœ… **Beautiful error UI** - Red-tinted messages with clear formatting
- âœ… **Rate limit guidance** - Helpful messages for API quota errors

#### Service Initialization
- âœ… **Race condition fix** - Renderer waits for services-ready event
- âœ… **Proper sequencing** - Services initialize before UI tries to use them
- âœ… **Status indicator** - Shows "Connected" only when LLM service is ready
- âœ… **Auto-start stream** - Screenshot stream starts automatically when ready

#### Gemini API Fixes
- âœ… **Schema cleaning** - Filters unsupported JSON Schema fields for Gemini
- âœ… **Function response format** - Correct format: `{ functionResponse: { name, response } }`
- âœ… **Function calls extraction** - Fixed getter function call
- âœ… **System prompt handling** - Prepends to first message instead of using systemInstruction

### Bug Fixes
- ğŸ› Fixed error messages being returned as successful responses
- ğŸ› Fixed retry button calling wrong function name
- ğŸ› Fixed status showing "Connected" before services ready
- ğŸ› Fixed screenshot service race condition
- ğŸ› Fixed MCP server port conflicts on restart
- ğŸ› Fixed DevTools opening in production mode

### Technical Changes
- Changed from StdioClientTransport to SSEClientTransport
- Added HTTP endpoint polling for server readiness detection
- Added signal handlers (SIGINT, SIGTERM) for cleanup
- LLM service now throws errors instead of returning error strings
- Added error object with status/statusText to IPC responses
- Improved error parsing in renderer with regex extraction

## Version 1.0.0 - Initial Release (2025-11-05)

### Features

#### Core Application
- âœ… ElectronJS-based desktop application
- âœ… Modern dark-themed UI with split-panel layout
- âœ… Chat interface on the left for conversational AI interaction
- âœ… Canvas panel on the right for real-time browser screenshots
- âœ… 15 FPS screenshot streaming from headless browser

#### LLM Integration
- âœ… **Dual LLM Support**: Choose between Claude or Gemini
  - Anthropic Claude 3.5 Sonnet with function calling
  - Google Gemini 2.0 Flash with function calling
- âœ… Automatic tool/function calling for browser automation
- âœ… Conversation history management
- âœ… Visual indicator showing active LLM provider

#### Browser Automation
- âœ… Integration with official `@playwright/mcp` package
- âœ… Headless Chrome browser automation
- âœ… Full access to Playwright MCP tools:
  - Navigation (goto, back, forward)
  - Element interaction (click, type, fill forms)
  - Screenshots and page snapshots
  - Console messages and network requests
  - Dialog handling
  - File uploads
  - JavaScript evaluation
  - And more...

#### User Experience
- âœ… Real-time status indicators
- âœ… FPS counter for screenshot stream
- âœ… Loading animations for AI responses
- âœ… Message timestamps
- âœ… Smooth animations and transitions
- âœ… Keyboard shortcuts (Enter to send, Shift+Enter for new line)

### Project Structure
```
conversational-playwright/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.js                   # Electron main process
â”‚   â”œâ”€â”€ preload.js                # IPC bridge
â”‚   â”œâ”€â”€ index.html                # UI structure
â”‚   â”œâ”€â”€ styles.css                # Styling
â”‚   â”œâ”€â”€ renderer.js               # Frontend logic
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ mcpService.js         # Playwright MCP client
â”‚       â”œâ”€â”€ llmService.js         # LLM integration (Claude/Gemini)
â”‚       â””â”€â”€ screenshotService.js  # Screenshot streaming
â”œâ”€â”€ package.json
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â””â”€â”€ CHANGELOG.md
```

### Configuration Options

#### LLM Provider Selection
- Set `LLM_PROVIDER=claude` for Anthropic Claude
- Set `LLM_PROVIDER=gemini` for Google Gemini

#### MCP Server Options
- Customizable via `MCP_SERVER_ARGS` in `.env`
- Supports all `@playwright/mcp` command-line options
- Default: headless mode

### Dependencies
- `electron` ^27.0.0
- `@anthropic-ai/sdk` ^0.27.0
- `@google/generative-ai` ^0.21.0
- `@playwright/mcp` latest
- `@modelcontextprotocol/sdk` ^0.5.0
- `dotenv` ^16.3.1

### Known Limitations
- Screenshot streaming requires browser to be navigated to a page
- First run may take longer as Playwright downloads browsers
- Network connectivity required for LLM API calls

### Future Enhancements (Potential)
- [ ] Support for additional LLM providers (OpenAI, etc.)
- [ ] Adjustable screenshot FPS
- [ ] Session recording and playback
- [ ] Multiple browser tabs support
- [ ] Custom browser profiles
- [ ] Screenshot annotation tools
- [ ] Export conversation history
- [ ] Dark/Light theme toggle
